{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fOHXFU8fc9ObKGUvKgAfqVC3EZFndKAD","timestamp":1738919351581},{"file_id":"1YuP-XPxo--AsDsaX0XI_2scCPADDT_jj","timestamp":1737006268380},{"file_id":"1iW7pcW3qg7uCrEvFypSfTYGPZgB06b7m","timestamp":1734406157963}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Text Classification (AI or Human Written) with Word2Vec and Machine Learning Models"],"metadata":{"id":"sFDzq_AwMmPV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uvtsIdqSCgW"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#Importing necessary libraries"],"metadata":{"id":"fjrDNSiFMbYw"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from gensim.models import Word2Vec\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import re\n","import joblib\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.naive_bayes import MultinomialNB"],"metadata":{"id":"K-oo-mp_Kdwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import ComplementNB\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n"],"metadata":{"id":"StnisAUqO5SZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Defining file paths for the training and testing datasets stored on Google Drive.\n"],"metadata":{"id":"e79aoatiS4nl"}},{"cell_type":"code","source":["train_path=\"/content/train_data.csv\"\n","test_path =\"/content/test_data.csv\""],"metadata":{"id":"RPs01OP_5WA_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load  datasets\n"],"metadata":{"id":"4fxttPOETB0e"}},{"cell_type":"code","source":["train_df = pd.read_csv(train_path)\n","test_df = pd.read_csv(test_path)"],"metadata":{"id":"4QfO2vIX5fFb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_df.columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CesZ0CCrlSD9","executionInfo":{"status":"ok","timestamp":1738918472270,"user_tz":-345,"elapsed":4,"user":{"displayName":"Nisha Pokharel","userId":"14609158184766218401"}},"outputId":"e86cb6fc-4344-4acc-cdf5-cc0d16ce138f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['text', 'label', 'character_count', 'word_count', 'sentence_count',\n","       'paragraph_count', 'stopword_count', 'unique_word_count', 'pos_counts',\n","       'sentiment_polarity', 'sentiment_subjectivity',\n","       'discourse_marker_count', 'vocab_size', 'sentence_complexity',\n","       'grammatical_mistakes', 'punctuation_count',\n","       'sentence_length_difference', 'type_token_ratio'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["train_df = train_df.drop(columns=['pos_counts'])\n","test_df = test_df.drop(columns=['pos_counts'])"],"metadata":{"id":"kNmhq5FZkwxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_text_train = train_df['text']\n","X_features_train = train_df.drop(columns=['text', 'label'])\n","y_train = train_df['label']"],"metadata":{"id":"ZzENet_9LRk-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting features and target for test_df\n","X_text_test = test_df['text']\n","X_features_test = test_df.drop(columns=['text', 'label'])\n","y_test = test_df['label']\n"],"metadata":{"id":"uS736ZRfLRoA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences = [text.split() for text in X_text_train]"],"metadata":{"id":"OO8zf-MNmkyJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train Word2Vec model\n","word2vec_model = Word2Vec(\n","    sentences=sentences,  # The input corpus, where each sentence is tokenized into a list of words.\n","    vector_size=100,         # The dimensionality of the word vectors (size of each word embedding).\n","    window=5,                # The maximum distance between the current and predicted word in a sentence (context window).\n","    min_count=2,             # Ignores words that appear less than 2 times in the corpus.\n","    workers=4,               # Number of worker threads used for training (parallelism).\n","    sg=0                     # Specifies the training algorithm: 0 for CBOW (Continuous Bag of Words), 1 for Skip-gram.\n",")"],"metadata":{"id":"lIFj_upOLRqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the Word2Vec model\n","word2vec_model.save(\"word2vec_model.model\")"],"metadata":{"id":"UCTvN3k4nCH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to create text embeddings by averaging word vectors\n","def text_to_vector(corpus, model):\n","    vectors = []\n","    for words in corpus:\n","        word_vecs = [model.wv[word] for word in words if word in model.wv]\n","        if len(word_vecs) > 0:\n","            vectors.append(np.mean(word_vecs, axis=0))\n","        else:\n","            vectors.append(np.zeros(model.vector_size))\n","    return np.array(vectors)"],"metadata":{"id":"XwtOBXUx6p-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert text to Word2Vec vectors\n","X_text_train_vectors = text_to_vector(X_text_train, word2vec_model)\n","X_text_test_vectors = text_to_vector(X_text_test, word2vec_model)\n"],"metadata":{"id":"2b5oVOEmMvnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine Word2Vec vectors with engineered features\n","X_train_combined = np.hstack((X_text_train_vectors, X_features_train))\n","X_test_combined = np.hstack((X_text_test_vectors, X_features_test))"],"metadata":{"id":"kdvuNR38Mvqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaler = MinMaxScaler()\n","X_train_vectors = scaler.fit_transform(X_train_combined)\n","X_test_vectors = scaler.transform(X_test_combined)"],"metadata":{"id":"QG-H79eZTSpQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["joblib.dump(scaler, 'minmax_scaler.joblib')  # Save the fitted scaler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BdlQtocLkWsv","executionInfo":{"status":"ok","timestamp":1738918584708,"user_tz":-345,"elapsed":18,"user":{"displayName":"Nisha Pokharel","userId":"14609158184766218401"}},"outputId":"53e0683e-2523-4c52-ff04-627aec4d29e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['minmax_scaler.joblib']"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["xgb_model = XGBClassifier(eval_metric='logloss', random_state=47)\n","svm_model = SVC(probability=True)\n","nb_model = MultinomialNB()\n","rf_model = RandomForestClassifier(random_state=47)"],"metadata":{"id":"H7miHFf_SAEH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = {\n","    'XGBoost': xgb_model,\n","    'SVM': svm_model,\n","    'Naive Bayes': nb_model,\n","    'Random Forest': rf_model\n","}"],"metadata":{"id":"bhE2HWOWM3j2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = []\n","\n","for model_name, model in models.items():\n","    # Train model\n","    model.fit(X_train_vectors, y_train)\n","\n","    # Predict on test data\n","    y_pred = model.predict(X_test_vectors)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    # Save the model using joblib\n","    joblib.dump(model, f\"{model_name}_model.joblib\")\n","\n","    results.append({\n","        'Model': model_name,\n","        'Accuracy': accuracy,\n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1 Score': f1\n","    })\n","\n"],"metadata":{"id":"0ZkyYI-WSQc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df = pd.DataFrame(results)\n","print(results_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S46QSzVaWOWv","executionInfo":{"status":"ok","timestamp":1738918629256,"user_tz":-345,"elapsed":17,"user":{"displayName":"Nisha Pokharel","userId":"14609158184766218401"}},"outputId":"c7c0406a-a2e3-4ac1-ce91-23eaf7a2a430"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["           Model  Accuracy  Precision    Recall  F1 Score\n","0        XGBoost  0.973425   0.963682  0.983621  0.973549\n","1            SVM  0.938706   0.926236  0.952586  0.939227\n","2    Naive Bayes  0.802400   0.826941  0.762069  0.793181\n","3  Random Forest  0.961852   0.955745  0.968103  0.961884\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iTIDekQTM3og"},"execution_count":null,"outputs":[]}]}